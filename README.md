# Look, Ask, Explain: The VQA Challenge

## Overview
This project explores Visual Question Answering (VQA) â€” a task that combines computer vision and natural language processing.
The objective is to develop an AI system that can answer natural-language questions about an image and visualize *why* it gave that answer.

## Goals
- Build a Vision+Language pipeline using BLIP, CLIP, or Flamingo-style models
- Train and test on VQA v2 or VizWiz datasets
- Visualize attention heatmaps for explainability


## Week 1: Getting Started
- Understand the VQA problem and datasets
- Run inference with a pre-trained BLIP model
- Visualize sample outputs
- Prepare the dataset directory structure

## Setup Instructions
```bash
git clone https://github.com/sh32dwe/WiDS-5.0-mentorship.git
cd WiDS-5.0-mentorship
pip install -r requirements.txt
```

## Expected Outcomes
- Working inference from a pre-trained model
- Understanding of dataset structure
- First visualization of model reasoning
